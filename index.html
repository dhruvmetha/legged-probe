<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PROBE: Proprioceptive Obstacle Detection and Estimation while Navigating in Clutter">
  <meta name="keywords" content="Navigation, Policy, PROBE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PROBE: Proprioceptive Obstacle Detection and Estimation while Navigating in Clutter</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/noun-dog-robot.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">PROBE: Proprioceptive Obstacle Detection and Estimation while Navigating in Clutter</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dhruvmetha.github.io/">Dhruv Metha Ramesh</a>,</span>
            <span class="author-block">
              <a href="https://linkedin.com/in/keskarshreesh">Shreesh Keskar</a>,</span>
            <span class="author-block">
              <a href="https://people.cs.rutgers.edu/~as2578/">Aravind Sivaramakrishnan</a>,
            </span></br>
            <span class="author-block">
              <a href="https://robotics.cs.rutgers.edu/pracsys/members/kostas-bekris/">Kostas E Bekris</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.rutgers.edu/people/professors/details/jingjin-yu">Jingjin Yu</a>,
            </span>
            <span class="author-block">
              <a href="http://www.abdeslam.net/">Abdeslam Boularias</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Rutgers, The State University of New Jersey</span>
          </div>

          <div style="padding-top: 8px;">
              <a target="_blank" href="https://github.com/dhruvmetha/probe" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In critical applications, including search-and-rescue in degraded environments, blockages
            can be prevalent and prevent the effective deployment of certain sensing modalities, particularly
            vision, due to occlusion and the constrained range of view of onboard camera sensors. To
            enable robots to tackle these challenges, we propose a new approach, Proprioceptive Obstacle Detection
            and Estimation while navigating in clutter (PROBE), that instead utilizes the robotâ€™s proprioception to
            infer the presence or the absence of occluded planar obstacles while predicting their dimensions and poses
            in SE(2). As a novel vision-free technique, PROBE simultaneously navigates in cluttered environments 
            and detects on the fly the presence and dimensions of unseen static/movable obstacles entirely through
            physical contact interactions. PROBE is a Transformer neural network that receives as inputs a history
            of applied torques and sensed whole-body movements of the robot and returns a parameterized representation
            of the obstacles in the environment. The effectiveness of PROBE is thoroughly evaluated on simulated environments
            in Isaac Gym and a real Unitree Go1 quadruped.
          </p>
        </div>
      </div>
    </div>
   
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">System Pipeline</h2>
          <img src="./static/images/InferencePipeline.png" alt="PROBE Inference Pipeline"
            id="inference-pipeline" style="border: 2px solid #000;">
        </div>
      </div>
    </div>

  </div>
  <div class="hero-body">
    
    <div class="title is-4" style="text-align:center">Real Robot Trials</div>

    <div style="padding-bottom: 8px;">
      For all the demonstrations below the ground truth obstacles are colored in yellow for movable and red for static. The predicted obstacles are colored in orange for yellow and blue for static. The robot pose is shown in green.
    </div>

    <div style="padding: 8px 0px">
      <div class="title is-5" style="width: 100%; padding: 4px 0px;">
        Category 1: Easy (One movable or one static obstacle)
      </div>
      <!-- <div style="padding-bottom: 8px;">
        The ground truth obstacles are colored in yellow for movable and red for static. The predicted obstacles are colored in orange for yellow and blue for static. The robot pose is shown in green.
      </div> -->
      <div style="display: inline-flex;">
        <div style="padding: 4px;">
          <video autoplay controls muted loop playsinline style="border: 2px solid #000; width:100%;">
            <source src="./static/videos/easy/demo_0.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div style="padding: 4px;">
          <video autoplay controls muted loop playsinline style="border: 2px solid #000; width:100%;">
            <source src="./static/videos/easy/demo_2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

    <div style="padding: 8px 0px">
      <div class="title is-5" style="width: 100%; padding: 4px 0px;">
        Category 2: Medium (One static behind one movable)
      </div>
      <!-- <div style="padding-bottom: 8px;">
        The ground truth obstacles are colored in yellow for movable and red for static. The predicted obstacles are colored in orange for yellow and blue for static. The robot pose is shown in green.
      </div> -->
      <div style="display: inline-flex;">
        <div style="padding: 4px;">
          <video autoplay controls muted loop playsinline style="border: 2px solid #000; width:100%;">
            <source src="./static/videos/medium/demo_3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div style="padding: 4px;">
          <video autoplay controls muted loop playsinline style="border: 2px solid #000; width:100%;">
            <source src="./static/videos/medium/demo_6.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div style="display: inline-flex;">
        <div style="padding: 4px;">
          <video autoplay controls muted loop playsinline style="border: 2px solid #000; width:100%;">
            <source src="./static/videos/medium/demo_17.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div style="padding: 4px;">
          <video autoplay controls muted loop playsinline style="border: 2px solid #000; width:100%;">
            <source src="./static/videos/medium/demo_20.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

    </div>

    <div style="padding: 8px 0px">
      <div class="title is-5" style="width: 100%; padding: 4px 0px;">
        Category 3: Hard (Two static obstacles behind one movable)
      </div>
      <div style="display: inline-flex;">
        <div style="padding: 4px;">
          <video autoplay controls muted loop playsinline style="border: 2px solid #000; width:100%;">
            <source src="./static/videos/hard/demo_4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div style="padding: 4px;">
          <video autoplay controls muted loop playsinline style="border: 2px solid #000; width:100%;">
            <source src="./static/videos/hard/demo_5.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Template obtained with thanks from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> page 
            under the <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
            4.0 International License</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
